---
title: "Medical Insurance Cost Classification"
subtitle: "Supervised Learning - Assignment 2"
author: "Cesaire Tobias"
date: "May 15, 2025"
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    lof: true
    lot: true
    documentclass: article
    geometry: margin=1in
    fontsize: 11pt
    linestretch: 1.2
execute:
  echo: false
  warning: false
  message: false
---

```{r setup, include=FALSE}
# Load required libraries
library(knitr)       # For dynamic report generation
library(dplyr)       # For data manipulation
library(ggplot2)     # For data visualization
library(kableExtra)  # For enhanced table formatting
library(scales)      # For formatting plot scales
library(corrplot)    # For correlation visualization
library(rpart)       # For decision trees
library(rpart.plot)  # For plotting decision trees
library(randomForest) # For random forest models
library(xgboost)     # For XGBoost models
library(caret)       # For model training and evaluation
library(pROC)        # For ROC curves
library(PRROC)       # For PR curves
library(ROCR)        # For ROC analysis
library(viridis)     # For better color palettes
library(glmnet)      # For LASSO regularization
library(tibble)      # For tidy tables

# Set seed for reproducibility
my_seed <- 9104

# Set default chunk options for the entire document
knitr::opts_chunk$set(
  echo = FALSE,        # Don't show code in final output
  warning = FALSE,     # Suppress warnings
  message = FALSE,     # Suppress messages
  fig.align = "center",  # Center figures
  fig.pos = "H",       # Position figures exactly here
  fig.width = 8,       # Set wider default figure width
  fig.height = 6,      # Set taller default figure height
  out.width = "95%",   # Set output figure width
  dpi = 300,           # Higher resolution
  results = "asis"     # Output results as-is
)

# Custom theme for consistent plot styling
my_theme <- theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5, color = "gray40"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.text = element_text(size = 10),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(fill = NA, color = "gray90"),
    plot.margin = unit(c(1, 1, 1, 1), "cm") # Using base unit function
  )

# Custom color palettes
charge_colors <- c("#5ab4ac", "#d8b365")  # For binary target
model_colors <- viridis(4, option = "plasma")  # For model comparison
```

```{r load-data}
# Define paths and constants
base_data_dir <- "./inst/extdata"
github_repo <- "https://github.com/ces0491/sl-assignment2.git"
github_data_url <- "https://github.com/ces0491/sl-assignment2/raw/refs/heads/main/data/insurance_data_A2.RData"

# Try to read the data from local files first
if (file.exists(file.path(base_data_dir, "insurance_A2.csv"))) {
  insurance_data <- read.csv(file.path(base_data_dir, "insurance_A2.csv"))
} else {
  # If local file doesn't exist, try to download from GitHub
  tryCatch({
    temp_file <- tempfile(fileext = ".RData")
    download.file(github_data_url, temp_file, mode = "wb")
    load(temp_file)
    # Assuming the RData file contains insurance_data object
    if (!exists("insurance_A2")) {
      stop("RData file doesn't contain insurance_data object")
    }
    unlink(temp_file)  # Clean up
    
    # Save the data locally for future runs
    if (!dir.exists(base_data_dir)) {
      dir.create(base_data_dir, recursive = TRUE)
    }
    write.csv(insurance_data, file.path(base_data_dir, "insurance_A2.csv"), row.names = FALSE)
  }, error = function(e) {
    stop("Error loading insurance data: ", e$message)
  })
}

# Initial inspection to verify data loaded correctly
if (nrow(insurance_data) == 0 || !("charges" %in% colnames(insurance_data))) {
  stop("Error loading insurance data")
}
```

# Introduction {#sec-introduction}

This report extends previous analysis of medical insurance costs by transitioning from regression to binary classification of insurance costs as either "high" or "low" based on patient characteristics. This approach provides a simplified risk assessment framework addressing key stakeholder needs.

Key questions addressed:

-   **Patients**: Which factors significantly increase likelihood of high charges?
-   **Insurers**: How can binary risk classification improve premium calculations?
-   **Policymakers**: Which factors should be targeted to reduce high-cost claims?

Dataset features include:

-   **age**: `Integer` - primary beneficiary's age
-   **sex**: `Factor` - gender (female/male)
-   **bmi**: `Continuous` - Body Mass Index
-   **children**: `Integer` - number of dependents
-   **smoker**: `Factor` - smoking status (yes/no)
-   **region**: `Factor` - US residential area (northeast, southeast, southwest, northwest)

The target variable **charges** has been transformed (external to this analysis) from a continuous dollar amount to binary ("high"/"low"). Four classification algorithms are implemented: L1-regularized (LASSO) logistic regression, classification tree, random forest, and XGBoost.

## Data Sources {#sec-data-sources}

The data can be accessed from:

- **GitHub Repository**: [sl-assignment2](https://github.com/ces0491/sl-assignment2.git)
- **Direct RData Link**: [insurance_data_A2.RData](https://github.com/ces0491/sl-assignment2/raw/refs/heads/main/data/insurance_data_A2.RData)

## Methodology Overview {#sec-methodology}

The model development approach comprises three phases:

1. **Training Phase**: The `insurance_A2.csv` dataset is split into training (80%) and internal validation (20%) sets.
2. **Model Selection Phase**: Models are evaluated on the internal validation set with emphasis on the F1 score.
3. **External Validation Phase**: The best model is then applied to a separate dataset (`A2_testing.csv`).

This approach minimizes over-fitting risk and provides a more robust assessment of model generalizability.

```{r data-preparation}
# Convert appropriate columns to factors
insurance_data <- insurance_data %>%
  mutate(
    sex = as.factor(sex),
    smoker = as.factor(smoker),
    region = as.factor(region),
    charges = as.factor(charges),
    children = as.integer(children)
  )

# Explicitly set reference levels for consistent encoding
insurance_data$charges <- relevel(insurance_data$charges, ref = "low")
insurance_data$sex <- relevel(insurance_data$sex, ref = "female")
insurance_data$smoker <- relevel(insurance_data$smoker, ref = "no")
insurance_data$region <- relevel(insurance_data$region, ref = "northeast")

# Split data into training and testing sets
set.seed(my_seed)
train_index <- createDataPartition(insurance_data$charges, p = 0.8, list = FALSE)
train_data <- insurance_data[train_index, ]
test_data <- insurance_data[-train_index, ]

# Verify encoding consistency in split data
stopifnot(
  all(levels(train_data$charges) == levels(insurance_data$charges)),
  all(levels(test_data$charges) == levels(insurance_data$charges))
)

# Store class distribution for consistent referencing
training_charges_dist <- table(train_data$charges)
training_charges_percent <- prop.table(training_charges_dist) * 100
```

# Exploratory Data Analysis {#sec-exploratory-data-analysis}

## Data Structure and Target Distribution {#sec-target-distribution}

```{r target-distribution, fig.height=5}
# Calculate the distribution of the target variable
charges_distribution <- table(insurance_data$charges)
charges_percent <- prop.table(charges_distribution) * 100

# Create bar plot
ggplot(data.frame(charges = insurance_data$charges), 
       aes(x = charges, fill = charges)) +
  geom_bar(width = 0.6, color = "white") +
  geom_text(stat = "count", 
            aes(label = paste0(round(..count../sum(..count..)*100, 1), "%")), 
            vjust = 1.5,
            color = "white",
            size = 5) +
  labs(title = "Distribution of Target Variable (Charges)",
       x = "Charge Category",
       y = "Count") +
  scale_fill_manual(values = charge_colors) +
  my_theme +
  theme(axis.text = element_text(size = 12),
        plot.margin = unit(c(1, 1, 1, 1), "cm")) +  # Increase margins
  coord_cartesian(clip = "off")  # Prevent clipping of elements
```

The target variable shows class imbalance with `r round(charges_percent["low"], 1)`% "low" and `r round(charges_percent["high"], 1)`% "high" charges. This imbalance makes F1 score a priority metric as it balances precision and recall, which is less sensitive to class imbalance than accuracy.

## Key Variable Relationships {#sec-key-relationships}

```{r fig-categorical-analysis, fig.cap="Proportion of Charges by Smoking Status", fig.height=5}
# Calculate proportions for smokers and non-smokers
smoker_props <- insurance_data %>%
  group_by(smoker, charges) %>%
  summarise(count = n()) %>%
  group_by(smoker) %>%
  mutate(
    prop = count / sum(count),
    percentage = prop * 100,
    formatted_pct = paste0(round(percentage, 1), "%")
  )

# Extract key values for dynamic text references
smoker_high_pct <- smoker_props %>% 
  filter(smoker == "yes" & charges == "high") %>% 
  pull(percentage)

smoker_low_pct <- smoker_props %>% 
  filter(smoker == "yes" & charges == "low") %>% 
  pull(percentage)

nonsmoker_high_pct <- smoker_props %>% 
  filter(smoker == "no" & charges == "high") %>% 
  pull(percentage)

nonsmoker_low_pct <- smoker_props %>% 
  filter(smoker == "no" & charges == "low") %>% 
  pull(percentage)

# Create bar plot for smoker status with dynamically calculated values
ggplot(insurance_data, aes(x = smoker, fill = charges)) +
  geom_bar(position = "fill", width = 0.7, color = "white") +
  geom_text(aes(label = percent(..count../tapply(..count.., ..x.., sum)[..x..])),
            position = position_fill(vjust = 0.5),
            stat = "count", 
            color = "white", 
            size = 4.5,
            fontface = "bold") +
  labs(title = "Proportion of Charges by Smoking Status",
       x = "Smoker",
       y = "Proportion") +
  scale_fill_manual(values = charge_colors) +
  scale_y_continuous(labels = percent, expand = c(0, 0)) +
  my_theme
```

**Key findings from variable analysis:**

- **Age**: Higher ages correlate with "high" charges in an approximately linear relationship
- **BMI**: "High" charges tend to have higher BMI values, with a potential non-linear relationship
- **Smoking status**: The strongest categorical predictor, with `r round(smoker_high_pct, 1)`% of smokers classified as "high" charges compared to only `r round(nonsmoker_high_pct, 1)`% of non-smokers (as shown in @fig-categorical-analysis)
- **Sex**: Only minor differences between males and females
- **Region**: Modest regional differences, with northeast showing slightly higher proportion of "high" charges
- **Children**: A slight trend toward higher charges for families with more children

```{r correlation-matrix, fig.cap="Correlation Matrix of Variables", fig.height=5}
# Convert target to numeric for correlation
insurance_numeric <- insurance_data
# Ensure consistent encoding - "high" = 1, "low" = 0
insurance_numeric$charges_numeric <- ifelse(insurance_data$charges == "high", 1, 0)
insurance_numeric$smoker_numeric <- ifelse(insurance_data$smoker == "yes", 1, 0)
insurance_numeric$sex_numeric <- ifelse(insurance_data$sex == "male", 1, 0)

# Create correlation matrix
corr_vars <- c("age", "bmi", "children", "smoker_numeric", "sex_numeric", "charges_numeric")
correlation_matrix <- cor(insurance_numeric[, corr_vars])

# Correlation plot
corrplot(correlation_matrix, 
         method = "circle", 
         type = "upper", 
         order = "hclust",
         tl.col = "black", 
         tl.srt = 45, 
         addCoef.col = "black",
         col = colorRampPalette(charge_colors)(200),
         diag = FALSE,
         title = "Correlation Matrix of Variables",
         mar = c(0, 0, 2, 0),
         number.cex = 0.9,
         tl.cex = 0.9)
```

**Correlation analysis confirms:**

- **Age** has the strongest correlation with high charges (`r round(correlation_matrix["age", "charges_numeric"], 2)`)
- **Smoking status** has the second strongest correlation (`r round(correlation_matrix["smoker_numeric", "charges_numeric"], 2)`)
- **BMI** shows moderate positive correlation (`r round(correlation_matrix["bmi", "charges_numeric"], 2)`)
- **Children** and **Sex** show weaker correlations
- Low multicollinearity among predictors is favorable for modeling

**Important interaction effects:**

- **Smoking and Age**: Smoking is such a dominant predictor that `r ifelse(smoker_high_pct == 100, "all", paste0(round(smoker_high_pct, 1), "% of"))` smokers fall into "high" charges category regardless of age
- **Smoking and BMI**: For non-smokers, higher BMI correlates more strongly with "high" charges

# Modeling {#sec-modeling}

## Logistic Regression with L1 Regularization {#sec-logistic}

```{r logistic-full}

# Verify factor levels before modeling
stopifnot(levels(train_data$charges)[1] == "low")

# Fit full logistic regression model with consistent encoding
logistic_full <- glm(charges ~ age + sex + bmi + children + smoker + region, 
                   family = binomial(link = "logit"), 
                   data = train_data)

# Print model summary for full logistic regression model
summary_table <- summary(logistic_full)

# Display coefficient table in more readable format
coef_table <- as.data.frame(summary_table$coefficients)
colnames(coef_table) <- c("Estimate", "Std. Error", "z value", "Pr(>|z|)")
coef_table$Significance <- ifelse(coef_table$`Pr(>|z|)` < 0.001, "***",
                              ifelse(coef_table$`Pr(>|z|)` < 0.01, "**",
                                ifelse(coef_table$`Pr(>|z|)` < 0.05, "*", "")))

kbl(
  coef_table,
  caption = "Full Logistic Regression Model Coefficients",
  format = "latex",
  booktabs = TRUE,
  align = "c",
  digits = 4
) %>%
  kable_styling(
    latex_options = c("HOLD_position", "scale_down"),
    font_size = 10,
    position = "center"
  ) %>%
  footnote(
    general = "Significance codes: *** p<0.001, ** p<0.01, * p<0.05",
    general_title = "Note: ",
    footnote_as_chunk = TRUE,
    escape = FALSE
  )

# Show model fit statistics
model_stats <- data.frame(
  Statistic = c("Null Deviance", "Residual Deviance", "AIC"),
  Value = c(summary_table$null.deviance, summary_table$deviance, summary_table$aic)
)

# kbl(
#   model_stats,
#   caption = "Logistic Regression Model Fit Statistics",
#   format = "latex",
#   booktabs = TRUE,
#   align = "lc",
#   digits = 2
# ) %>%
#   kable_styling(
#     latex_options = c("HOLD_position"),
#     font_size = 10,
#     position = "center"
#   )
```

The full logistic regression model before regularization reveals several significant predictors. Most notably, smoking status exhibits an extremely large positive coefficient (`r round(logistic_full$coefficients["smokeryes"],3)`), indicating smokers have dramatically higher odds of being in the "high" charges category. Age also shows a significant positive effect (`r round(logistic_full$coefficients["age"],3)`), with each additional year increasing the log-odds of high charges. Regional variables demonstrate statistically significant negative coefficients compared to the northeast reference region, suggesting geographical variation in insurance costs.
However, this unregularized model may be susceptible to overfitting, particularly with our relatively small dataset. To address this concern, we apply L1 regularization (LASSO) to perform variable selection and coefficient shrinkage simultaneously.

```{r lasso-model, fig.height=5}
# Prepare data for LASSO
set.seed(my_seed)
x_train <- model.matrix(charges ~ ., data = train_data)[, -1]  # Remove intercept
# Ensure consistent encoding: "high" = 1, "low" = 0
y_train <- ifelse(train_data$charges == "high", 1, 0)
x_test <- model.matrix(charges ~ ., data = test_data)[, -1]

# Find optimal lambda using cross-validation
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial", nfolds = 10)

# Fit LASSO model with optimal lambda
lasso_model <- glmnet(x_train, y_train, alpha = 1, family = "binomial", 
                      lambda = cv_lasso$lambda.min)

# Get non-zero coefficients
lasso_coef <- as.matrix(coef(lasso_model))
lasso_coef_df <- data.frame(
  Variable = rownames(lasso_coef),
  Coefficient = as.vector(lasso_coef)
) %>%
  filter(Coefficient != 0) %>%
  arrange(desc(abs(Coefficient)))

# Display non-zero coefficients
kbl(
  lasso_coef_df,  # Show all non-zero coefficients
  caption = "LASSO Model Non-Zero Coefficients",
  format = "latex",
  booktabs = TRUE,
  align = "c",
  digits = 3
) %>%
  kable_styling(
    latex_options = c("HOLD_position", "scale_down"),
    font_size = 10,
    position = "center"
  )

# Calculate model metrics for LASSO
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
lasso_pred <- ifelse(lasso_pred_prob > 0.5, "high", "low")
lasso_conf_matrix <- confusionMatrix(factor(lasso_pred, levels = c("low", "high")), 
                                    test_data$charges, 
                                    positive = "high")
```

LASSO regularization approach identifies key predictors while reducing overfitting.

The results show:

- **Smoking status** is the strongest predictor with a coefficient of `r round(lasso_coef_df$Coefficient[lasso_coef_df$Variable == "smokeryes"], 3)`
- **Regional variables** show moderate negative effects (with reference to northeast): southeast (`r round(lasso_coef_df$Coefficient[lasso_coef_df$Variable == "regionsoutheast"], 3)`), southwest (`r round(lasso_coef_df$Coefficient[lasso_coef_df$Variable == "regionsouthwest"], 3)`), and northwest (`r round(lasso_coef_df$Coefficient[lasso_coef_df$Variable == "regionnorthwest"], 3)`)
- **Children** has a stronger linear effect (`r round(lasso_coef_df$Coefficient[lasso_coef_df$Variable == "children"], 3)`) than **age** (`r round(lasso_coef_df$Coefficient[lasso_coef_df$Variable == "age"], 3)`)
- **BMI** shows a small but positive effect (`r round(lasso_coef_df$Coefficient[lasso_coef_df$Variable == "bmi"], 3)`)

## Classification Tree {#sec-tree}

```{r fig-classification-tree, fig.height=5.5, fig.cap="Pruned Classification Tree for Insurance Charges"}
# Verify consistent factor encoding before tree modeling
stopifnot(levels(train_data$charges)[1] == "low")

# Fit and cross-validate classification tree
set.seed(my_seed)
tree_cv <- rpart(charges ~ age + sex + bmi + children + smoker + region,
                data = train_data,
                method = "class",
                control = rpart.control(cp = 0.001, xval = 10))

# Find the optimal cp
opt_cp <- tree_cv$cptable[which.min(tree_cv$cptable[,"xerror"]),"CP"]

# Prune the tree
pruned_tree <- prune(tree_cv, cp = opt_cp)

# Plot the pruned tree
rpart.plot(pruned_tree, 
           extra = 106,
           box.palette = charge_colors,
           branch.lty = 1,
           shadow.col = "gray90",
           nn = TRUE,
           under = TRUE,
           fallen.leaves = TRUE,
           main = "Pruned Classification Tree",
           cex.main = 1.2,
           cex = 1) # Increase text size

# Calculate model metrics
tree_pred <- predict(pruned_tree, newdata = test_data, type = "class")
tree_conf_matrix <- confusionMatrix(tree_pred, test_data$charges, positive = "high")

# Extract information from the tree
root_node <- pruned_tree$frame[1,]
root_class <- ifelse(root_node$yval == 1, "low", "high")
root_prob <- round(root_node$yval2[,5], 2)

# Function to extract information about a specific node
get_node_info <- function(node_number) {
  node_data <- pruned_tree$frame[node_number,]
  node_class <- ifelse(node_data$yval == 1, "low", "high")
  node_probs <- node_data$yval2[,4:5]
  node_prob_class <- node_probs[node_class]
  node_percent <- round(node_data$yval2[,5], 2)
  
  return(list(
    class = node_class,
    prob = node_prob_class,
    percent = node_percent
  ))
}

# Extract split variables and values
splits <- pruned_tree$splits
var_names <- row.names(splits)
split_points <- splits[,4]

# Get node information
node2_info <- get_node_info(2)
node3_info <- get_node_info(3)
node4_info <- get_node_info(4)
node5_info <- get_node_info(5)
node6_info <- get_node_info(6)
node7_info <- get_node_info(7)
node8_info <- get_node_info(8)
node9_info <- get_node_info(9)

# Get percentages of observations
node_percentages <- pruned_tree$frame$yval2[,5]

first_split_var <- row.names(pruned_tree$splits)[1]
first_split_value <- pruned_tree$splits[1,4]

smoker_split_idx <- which(row.names(pruned_tree$splits) == "smoker")
if(length(smoker_split_idx) > 0) {
  smoker_split_value <- pruned_tree$splits[smoker_split_idx[1],4]
}

age_splits <- which(row.names(pruned_tree$splits) == "age")
if(length(age_splits) > 1) {
  second_age_split_value <- pruned_tree$splits[age_splits[2],4]
}

region_split_idx <- which(grepl("region", row.names(pruned_tree$splits)))
if(length(region_split_idx) > 0) {
  region_split_value <- row.names(pruned_tree$splits)[region_split_idx[1]]
  region_split_value <- gsub("region=", "", region_split_value)
}
```

The classification tree (@fig-classification-tree) reveals key decision rules:

1. **Age is the primary predictor**: The model first splits on age, with `r ceiling(first_split_value)` being a critical threshold.

2. **For younger individuals (< `r ceiling(first_split_value)`)**, smoking status is the most important factor:
   - Non-smokers are generally low risk (`r round((1-node3_info$percent)*100)`% probability)
   - Smokers are high risk with `r node4_info$percent*100`% certainty in this model

3. **For older individuals ($\ge$ `r ceiling(first_split_value)`)**:
   - Those `r ceiling(second_age_split_value)` and older are almost certainly high risk (`r node9_info$percent*100`% probability)
   - For the middle age band (`r ceiling(first_split_value)`-`r ceiling(second_age_split_value-1)`), geographical region matters:
     - Being in the southwest region is associated with lower risk
     - Other regions have a higher risk (`r node8_info$percent*100`% probability)

This tree  captures complex interaction effects among variables. The pathways reveal that while smoking is a very strong predictor, age is the primary decision factor in the tree model, with smoking becoming decisive for younger patients.

## Random Forest {#sec-random-forest}

```{r random-forest, fig.cap="Random Forest Variable Importance", fig.height=5}
# Verify consistent factor encoding before random forest modeling
stopifnot(levels(train_data$charges)[1] == "low")

# Tune mtry parameter
set.seed(my_seed)
mtry_values <- c(2, 3, 4)
rf_tuning <- lapply(mtry_values, function(m) {
  model <- randomForest(charges ~ age + sex + bmi + children + smoker + region,
                       data = train_data,
                       mtry = m,
                       ntree = 500,
                       importance = TRUE)
  oob_err <- model$err.rate[nrow(model$err.rate), "OOB"]
  return(data.frame(mtry = m, oob_error = oob_err))
})

rf_tuning_results <- do.call(rbind, rf_tuning)

# Select best mtry
best_mtry <- rf_tuning_results$mtry[which.min(rf_tuning_results$oob_error)]

# Display tuning results
# kbl(
#   rf_tuning_results,
#   caption = "Random Forest Hyperparameter Tuning Results",
#   format = "latex",
#   booktabs = TRUE,
#   align = "c",
#   digits = 4
# ) %>%
#   kable_styling(
#     latex_options = c("HOLD_position"),
#     font_size = 10,
#     position = "center"
#   ) %>%
#   row_spec(which.min(rf_tuning_results$oob_error), bold = TRUE, background = "#E8F4F8") # Highlight best row

# Fit final random forest model with best mtry
set.seed(my_seed)
rf_model <- randomForest(charges ~ age + sex + bmi + children + smoker + region,
                        data = train_data,
                        mtry = best_mtry,
                        ntree = 500,
                        importance = TRUE)

# Extract Variable importance - using the correct method for randomForest objects
importance_df <- as.data.frame(randomForest::importance(rf_model))
importance_df$Variable <- rownames(importance_df)
importance_df <- importance_df[order(importance_df$MeanDecreaseGini, decreasing = TRUE), ]

# Create a plot of variable importance
ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "#5B9BD5") +
  geom_text(aes(label = round(MeanDecreaseGini, 1)), hjust = -0.2, size = 4) +
  coord_flip() +
  labs(title = "Random Forest Variable Importance",
       x = "Variable",
       y = "Mean Decrease in Gini Index") +
  my_theme +
  theme(
    panel.grid.major.y = element_blank(),
    plot.margin = unit(c(1, 2, 1, 1), "cm"),  # Increase right margin to prevent label cutoff
    axis.title.x = element_text(margin = grid::unit(c(10, 0, 0, 0), "pt"))
  ) +
  scale_y_continuous(limits = c(0, 230), expand = expansion(mult = c(0, 0.15)))  # Expand y-axis range to fit labels

# Calculate model metrics
rf_pred <- predict(rf_model, newdata = test_data)
rf_conf_matrix <- confusionMatrix(rf_pred, test_data$charges, positive = "high")
```

**mtry parameter selection rationale:**

- With 6 predictor variables, we tested mtry values of 2, 3, and 4, which represent approximately 1/3, 1/2, and 2/3 of the total features.
- This range spans from the default classification setting (sqrt(p) ≈ 2.45 for p=6) to a more aggressive feature sampling approach.
- Lower mtry values (e.g., 2) create more diverse trees with less correlation, potentially better for capturing non-linear patterns.
- Higher mtry values (e.g., 4) provide more candidate variables at each split, potentially capturing more complex relationships.
- Through 10-fold cross-validation, mtry = `r best_mtry` minimized out-of-bag error rate (`r round(min(rf_tuning_results$oob_error), 4)`), balancing between tree diversity and predictive power

**ntree parameter justification:**

- We selected 500 trees as this provides sufficient ensemble size to stabilize predictions, balancing computational efficiency with model stability and accuracy.

Random forest hyperparameter tuning identified mtry = `r best_mtry` as optimal. The variable importance pattern reveals a different perspective than the LASSO model:

- **Age** is ranked as the dominant predictor with Gini importance of `r round(importance_df$MeanDecreaseGini[importance_df$Variable == "age"], 1)`
- **Smoker status** is second most important with importance of `r round(importance_df$MeanDecreaseGini[importance_df$Variable == "smoker"], 1)`
- **BMI** ranked third with importance of `r round(importance_df$MeanDecreaseGini[importance_df$Variable == "bmi"], 1)`
- Other variables showing lower importance

This variable importance ranking differs from the LASSO model's ranking, suggesting that when non-linear relationships are considered, age emerges as more important than smoking status across the entire dataset, even though smoking creates a more dramatic binary separation.

## XGBoost Model {#sec-xgboost}

```{r xgboost-model, fig.cap="XGBoost Feature Importance", fig.height=5, results='hide'}
# Prepare data for XGBoost with consistent encoding
train_x <- model.matrix(charges ~ . - 1, data = train_data)
# Ensure consistent encoding: "high" = 1, "low" = 0
train_y <- ifelse(train_data$charges == "high", 1, 0)
test_x <- model.matrix(charges ~ . - 1, data = test_data)
test_y <- ifelse(test_data$charges == "high", 1, 0)

# Use the same seed (9104) as in the main analysis
set.seed(my_seed)

# Step 1: Define parameter grid with reasoned choices
param_grid <- expand.grid(
  eta = c(0.01, 0.1, 0.3),
  max_depth = c(2, 3, 5),
  subsample = c(0.6, 0.7, 0.9),
  colsample_bytree = c(0.6, 0.8, 1.0)
)

# Step 2: Create cross-validation folds
n_folds <- 5
set.seed(my_seed)
cv_folds <- createFolds(train_y, k = n_folds, returnTrain = TRUE)

# Step 3: Perform grid search with cross-validation
results <- data.frame()

# Capture and suppress output
invisible(capture.output({
  # Loop through parameter combinations
  for (i in 1:nrow(param_grid)) {
    current_params <- list(
      objective = "binary:logistic",
      eval_metric = "logloss",
      eta = param_grid$eta[i],
      max_depth = param_grid$max_depth[i],
      subsample = param_grid$subsample[i],
      colsample_bytree = param_grid$colsample_bytree[i]
    )
    
    # Performance tracking for this parameter set
    cv_aucs <- numeric(n_folds)
    cv_f1s <- numeric(n_folds)
    cv_iterations <- numeric(n_folds)
    
    # Cross-validation loop
    for (fold in 1:n_folds) {
      # Split data
      train_indices <- cv_folds[[fold]]
      fold_train_x <- train_x[train_indices, ]
      fold_train_y <- train_y[train_indices]
      fold_valid_x <- train_x[-train_indices, ]
      fold_valid_y <- train_y[-train_indices]
      
      # Create DMatrix objects
      dtrain_fold <- xgb.DMatrix(data = fold_train_x, label = fold_train_y)
      dvalid_fold <- xgb.DMatrix(data = fold_valid_x, label = fold_valid_y)
      
      # Train with early stopping
      xgb_cv <- xgb.train(
        params = current_params,
        data = dtrain_fold,
        nrounds = 200,
        watchlist = list(valid = dvalid_fold),
        early_stopping_rounds = 20,
        verbose = 0
      )
      
      # Get predictions
      pred_prob <- predict(xgb_cv, dvalid_fold)
      pred_class <- ifelse(pred_prob > 0.5, 1, 0)
      
      # Calculate metrics
      conf_matrix <- table(fold_valid_y, pred_class)
      if (dim(conf_matrix)[1] == 2 && dim(conf_matrix)[2] == 2) {
        # Calculate precision, recall, F1
        precision <- conf_matrix[2,2] / sum(conf_matrix[,2])
        recall <- conf_matrix[2,2] / sum(conf_matrix[2,])
        f1 <- 2 * precision * recall / (precision + recall)
        
        # Get AUC
        pred_obj <- prediction(pred_prob, fold_valid_y)
        auc <- performance(pred_obj, "auc")@y.values[[1]]
        
        # Store metrics
        cv_aucs[fold] <- auc
        cv_f1s[fold] <- f1
        cv_iterations[fold] <- xgb_cv$best_iteration
      } else {
        # Handle case where confusion matrix doesn't have both classes
        cv_aucs[fold] <- NA
        cv_f1s[fold] <- NA
        cv_iterations[fold] <- xgb_cv$best_iteration
      }
    }
    
    # Calculate average metrics across folds
    mean_auc <- mean(cv_aucs, na.rm = TRUE)
    mean_f1 <- mean(cv_f1s, na.rm = TRUE)
    mean_iteration <- round(mean(cv_iterations, na.rm = TRUE))
    
    # Store results
    result_row <- cbind(
      param_grid[i,],
      mean_auc = mean_auc,
      mean_f1 = mean_f1,
      mean_iteration = mean_iteration
    )
    results <- rbind(results, result_row)
  }
}))

# Step 4: Find best parameters based on F1 score
results <- results %>% arrange(desc(mean_f1))
best_result <- results[1,]

# Step 5: Train final model with best parameters
final_params <- list(
  objective = "binary:logistic",
  eval_metric = "logloss",
  eta = best_result$eta,
  max_depth = best_result$max_depth,
  subsample = best_result$subsample,
  colsample_bytree = best_result$colsample_bytree,
  tree_method = "hist",          # Use histogram-based algorithm (faster than 'exact')
  grow_policy = "lossguide",     # Grow by highest loss change (faster)
  nthread = parallel::detectCores() - 1  # Use multiple CPU
)

# Use the entire training set to fit the final model
dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dvalid <- xgb.DMatrix(data = test_x, label = test_y)

# Train final model with best parameters and optimal number of rounds
xgb_model <- xgb.train(
  params = final_params,
  data = dtrain,
  nrounds = best_result$mean_iteration,
  watchlist = list(train = dtrain, valid = dvalid),
  verbose = 0
)
```

```{r xgboost-visualization, fig.cap="XGBoost Feature Importance", fig.height=5}
# Feature importance
xgb_importance <- xgb.importance(feature_names = colnames(train_x), model = xgb_model)

# Visualize feature importance
ggplot(xgb_importance[1:6,], aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_col(fill = "#8A4B08") +
  geom_text(aes(label = round(Gain, 3)), hjust = -0.2, size = 4) +
  coord_flip() +
  labs(
    title = "XGBoost Feature Importance",
    x = "Feature",
    y = "Gain (Contribution to Model)"
  ) +
  my_theme +
  theme(
    panel.grid.major.y = element_blank(),
    plot.margin = unit(c(1, 2, 1, 1), "cm")  # Increase right margin to prevent label cutoff
  ) +
  scale_y_continuous(limits = c(0, 0.55), expand = expansion(mult = c(0, 0.15)))  # Expand y-axis range

# Calculate model metrics with consistent encoding
xgb_prob <- predict(xgb_model, dvalid)
xgb_pred <- ifelse(xgb_prob > 0.5, "high", "low")

# Ensure consistency in evaluation
xgb_conf_matrix <- confusionMatrix(
  factor(xgb_pred, levels = c("low", "high")), 
  factor(ifelse(test_y == 1, "high", "low"), levels = c("low", "high")), 
  positive = "high"
)
```

XGBoost combines sequential trees with each correcting errors of previous trees. The optimized parameters from our tuning process are:

- **Learning rate**: `r final_params$eta`
- **Max depth**: `r final_params$max_depth`
- **Subsample ratio**: `r final_params$subsample`
- **Column sample ratio**: `r final_params$colsample_bytree`

The feature importance analysis shows that age has the highest gain (`r round(xgb_importance$Gain[xgb_importance$Feature == "age"], 3)`), followed by smoking status (`r round(xgb_importance$Gain[xgb_importance$Feature == "smokeryes"], 3)`). Similar to the Random Forest model, XGBoost ranks age as more important than smoking status in terms of overall contribution to model performance, though the two models differ in their ranking of secondary predictors. In the XGBoost model, children has a gain of `r round(xgb_importance$Gain[xgb_importance$Feature == "children"], 3)`, placing it higher than BMI with a gain of `r round(xgb_importance$Gain[xgb_importance$Feature == "bmi"], 3)`.

# Model Evaluation and Comparison {#sec-evaluation}

```{r model-comparison, fig.cap="ROC Curves for All Models", fig.height=7, fig.width=9}
# Extract key metrics from confusion matrices
extract_metrics <- function(conf_matrix) {
  return(c(
    Accuracy = conf_matrix$overall["Accuracy"],
    Sensitivity = conf_matrix$byClass["Sensitivity"],
    Specificity = conf_matrix$byClass["Specificity"],
    Precision = conf_matrix$byClass["Pos Pred Value"],
    F1_Score = conf_matrix$byClass["F1"]
  ))
}

# Compute AUC for all models
# LASSO
lasso_pred_obj <- prediction(lasso_pred_prob, as.numeric(test_data$charges == "high")) # ensures we're passing numeric values (0/1) to the prediction function rather than factor levels.
lasso_auroc <- performance(lasso_pred_obj, "auc")@y.values[[1]]
# Properly calculate AUPRC for LASSO
lasso_pr <- pr.curve(
  scores.class0 = lasso_pred_prob[test_data$charges == "high"],
  scores.class1 = lasso_pred_prob[test_data$charges == "low"],
  curve = TRUE
)
lasso_auprc <- lasso_pr$auc.integral

# Classification Tree
tree_pred_prob <- predict(pruned_tree, newdata = test_data, type = "prob")[, "high"]
tree_pred_obj <- prediction(tree_pred_prob, as.numeric(test_data$charges == "high"))
tree_auroc <- performance(tree_pred_obj, "auc")@y.values[[1]]

# calculate AUPRC for Tree
tree_pr <- pr.curve(
  scores.class0 = tree_pred_prob[test_data$charges == "high"],
  scores.class1 = tree_pred_prob[test_data$charges == "low"],
  curve = TRUE
)
tree_auprc <- tree_pr$auc.integral

# Random Forest
rf_pred_prob <- predict(rf_model, newdata = test_data, type = "prob")[, "high"]
rf_pred_obj <- prediction(rf_pred_prob, as.numeric(test_data$charges == "high"))
rf_auroc <- performance(rf_pred_obj, "auc")@y.values[[1]]

# calculate AUPRC for Random Forest
rf_pr <- pr.curve(
  scores.class0 = rf_pred_prob[test_data$charges == "high"],
  scores.class1 = rf_pred_prob[test_data$charges == "low"],
  curve = TRUE
)
rf_auprc <- rf_pr$auc.integral

# XGBoost - ensure consistent encoding 
# Use test_y directly since it's already encoded as 1 = high, 0 = low
xgb_pred_obj <- prediction(xgb_prob, test_y)
xgb_auroc <- performance(xgb_pred_obj, "auc")@y.values[[1]]

# calculate AUPRC for XGBoost
xgb_pr <- pr.curve(
  scores.class0 = xgb_prob[test_y == 1],
  scores.class1 = xgb_prob[test_y == 0],
  curve = TRUE
)
xgb_auprc <- xgb_pr$auc.integral

# Combine all metrics
all_metrics <- rbind(
  "LASSO Logistic" = c(extract_metrics(lasso_conf_matrix), AUROC = lasso_auroc, AUPRC = lasso_auprc),
  "Classification Tree" = c(extract_metrics(tree_conf_matrix), AUROC = tree_auroc, AUPRC = tree_auprc),
  "Random Forest" = c(extract_metrics(rf_conf_matrix), AUROC = rf_auroc, AUPRC = rf_auprc),
  "XGBoost" = c(extract_metrics(xgb_conf_matrix), AUROC = xgb_auroc, AUPRC = xgb_auprc)
  ) %>% as.data.frame() %>% 
  setNames(c("Accuracy", "Sensitivity", "Specificity", "Precision", "F1_Score", "AUROC", "AUPRC"))

# Find the best model for each metric for dynamic references
best_f1_model <- rownames(all_metrics)[which.max(all_metrics$F1_Score)]
best_f1_score <- max(all_metrics$F1_Score)
best_accuracy_model <- rownames(all_metrics)[which.max(all_metrics$Accuracy)]
best_accuracy_score <- max(all_metrics$Accuracy)
best_auroc_model <- rownames(all_metrics)[which.max(all_metrics$AUROC)]
best_auroc_score <- max(all_metrics$AUROC)
best_auprc_model <- rownames(all_metrics)[which.max(all_metrics$AUPRC)]
best_auprc_score <- max(all_metrics$AUPRC)

# Format comparison table
kbl(
  rownames_to_column(all_metrics, "Model"),
  caption = "Comparison of Model Performance Metrics",
  format = "latex",
  booktabs = TRUE,
  align = c("l", rep("c", 7)),
  digits = 3
) %>%
  kable_styling(
    latex_options = c("striped", "scale_down", "HOLD_position"),
    font_size = 10,
    position = "center"
  ) %>%
  row_spec(which.max(all_metrics$F1_Score) + 1, bold = TRUE, background = "#E8F4F8") %>%
  column_spec(1, bold = TRUE)

# Extract ROC curve data for each model
roc_data <- data.frame(
  Model = character(),
  FPR = numeric(),
  TPR = numeric(),
  AUC = numeric(),
  stringsAsFactors = FALSE
)

# Ensure all prediction objects use the same reference format
models <- list(
  "XGBoost" = list(pred_prob = xgb_prob, actual = test_y),
  "Random Forest" = list(pred_prob = rf_pred_prob, actual = as.numeric(test_data$charges == "high")),
  "LASSO Logistic" = list(pred_prob = lasso_pred_prob, actual = as.numeric(test_data$charges == "high")),
  "Classification Tree" = list(pred_prob = tree_pred_prob, actual = as.numeric(test_data$charges == "high"))
)

# Create prediction objects and extract ROC data
for (model_name in names(models)) {
  # Create prediction object
  pred_obj <- prediction(models[[model_name]]$pred_prob, models[[model_name]]$actual)
  
  # Calculate AUC
  auc <- performance(pred_obj, "auc")@y.values[[1]]
  
  # Extract ROC performance data
  roc_perf <- performance(pred_obj, "tpr", "fpr")
  
  # Ensure we get the full curve from (0,0) to (1,1)
  model_data <- data.frame(
    Model = model_name,
    FPR = roc_perf@x.values[[1]],
    TPR = roc_perf@y.values[[1]],
    AUC = auc
  )
  
  roc_data <- rbind(roc_data, model_data)
}

# Create more aesthetic model labels with AUC values
roc_data$ModelLabel <- paste0(roc_data$Model, " (AUC = ", round(roc_data$AUC, 3), ")")
roc_data$ModelLabel <- factor(roc_data$ModelLabel, levels = unique(roc_data$ModelLabel))

# Custom color palette for better contrast
custom_colors <- c("#0072B2", "#D55E00", "#009E73", "#CC79A7")

# Create enhanced ROC curve plot with ggplot
roc_plot <- ggplot(roc_data, aes(x = FPR, y = TPR, color = ModelLabel)) +
  # Add reference diagonal line
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50", size = 0.8, alpha = 0.7) +
  # Add ROC curves
  geom_line(size = 1.5, alpha = 0.9) +
  # Add labels
  labs(
    title = "ROC Curves for All Models",
    subtitle = "Performance comparison of classification models",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    color = "Model"
  ) +
  # Ensure equal aspect ratio for proper display of ROC
  coord_equal(xlim = c(0, 1), ylim = c(0, 1)) +
  # Apply custom color palette
  scale_color_manual(values = custom_colors) +
  # Apply custom theme
  my_theme +
  # We'll add the annotation text with corrected placement
  theme(
    legend.position = "bottom",
    legend.box = "horizontal",
    legend.margin = grid::unit(c(10, 0, 0, 0), "pt"),
    plot.margin = unit(c(1, 1, 2, 1), "cm")  # Increase bottom margin for legend
  ) +
  # Add annotation with better placement
  annotate(
    "text", 
    x = 0.25, 
    y = 0.75, 
    label = "Models with curves closer to the\ntop-left corner perform better",
    size = 4, 
    fontface = "italic", 
    hjust = 0, 
    color = "gray40"
  )

roc_plot
```

The evaluation reveals different strengths across models:

1. **F1 Score**: `r best_f1_model` achieves highest F1 score (`r round(best_f1_score, 3)`), best balancing precision and recall, which is particularly important given our class imbalance
2. **Accuracy**: `r best_accuracy_model` leads in overall accuracy at `r round(best_accuracy_score, 3)`
3. **AUROC**: `r best_auroc_model` and Classification Tree both perform strongly in Area Under ROC Curve metrics (around `r round(best_auroc_score, 3)`)
4. **AUPRC**: `r best_auprc_model` shows strongest performance in Area Under Precision-Recall Curve at `r round(best_auprc_score, 3)`

Looking across metrics, we observe that tree-based models generally outperform logistic regression, highlighting the importance of capturing non-linear relationships and interactions present in the data. The relative performance varies by metric, requiring consideration of stakeholder priorities when selecting a final model.

The following analysis provides insight into why different models rank variables differently. While smoking creates a perfect binary separation for one group of patients, age provides more discriminative power across the entire dataset, particularly for non-smokers. Tree-based models can capture this complex relationship better than linear models like logistic regression.

```{r partial-dependence, fig.height=6, fig.width=9, fig.cap="Partial Dependence Plot: Age Effect by Smoking Status"}
# Create a function to make predictions that ensures factor levels match the training data
predict_with_factors <- function(model, new_data) {
  # Ensure all factor variables have the same levels as in the training data
  for (col in names(new_data)) {
    if (col %in% names(train_data) && is.factor(train_data[[col]])) {
      # Convert column to factor with same levels as in training data
      new_data[[col]] <- factor(new_data[[col]], levels = levels(train_data[[col]]))
    }
  }
  
  # Verify consistent encoding for charges if present
  if ("charges" %in% names(new_data) && is.factor(new_data$charges)) {
    stopifnot(levels(new_data$charges)[1] == "low")
  }
  
  # Make prediction
  if (inherits(model, "randomForest")) {
    return(predict(model, newdata = new_data, type = "prob")[, "high"])
  } else if (inherits(model, "xgb.Booster")) {
    # For XGBoost, need to convert to matrix first
    mat <- model.matrix(~ age + sex + bmi + children + smoker + region - 1, data = new_data)
    return(predict(model, newdata = mat))
  } else {
    # For other models, use standard predict
    return(predict(model, newdata = new_data, type = "prob")[, "high"])
  }
}

# Create partial dependence data
pdp_data <- expand.grid(
  age = seq(min(train_data$age), max(train_data$age), length.out = 50),
  smoker = c("yes", "no")
)

# Add constant values for other variables
pdp_data$bmi <- mean(train_data$bmi)
pdp_data$children <- median(train_data$children)
pdp_data$sex <- factor("male", levels = levels(train_data$sex))
pdp_data$region <- factor("northeast", levels = levels(train_data$region))
pdp_data$smoker <- factor(pdp_data$smoker, levels = levels(train_data$smoker))

# Generate predictions using the random forest model
pdp_data$probability <- predict_with_factors(rf_model, pdp_data)

# Create the partial dependence plot
ggplot(pdp_data, aes(x = age, y = probability, color = smoker)) +
  geom_line(size = 1.5) +
  labs(title = "Effect of Age on High Charge Probability by Smoking Status",
       subtitle = "Based on Random Forest model",
       x = "Age", 
       y = "Probability of High Charges") +
  scale_color_manual(values = c("no" = "#5ab4ac", "yes" = "#d8b365"),
                     labels = c("no" = "Non-smoker", "yes" = "Smoker")) +
  scale_x_continuous(breaks = seq(20, 65, by = 5)) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1)) +
  my_theme +
  # We'll add the annotation text once we see how the plot looks
  theme(legend.position = "bottom")
```

Partial dependence analysis using the Random Forest model reveals key patterns that help explain the model performance:

1. **Smoking Effect**: Smoking has a dramatic impact on the probability of high charges, showing a clear separation between smokers and non-smokers. Smokers consistently maintain a high probability (near 1.0) regardless of age.

2. **Age Effect**: For non-smokers, age shows a more complex, non-linear relationship with the probability of high charges:
   - Ages 20-40: Moderate probability with some variability
   - Ages 40-45: Sharp increase in probability 
   - Ages 45+: Nearly all classified as high charges

3. **Age-Smoking Interaction**: The visualization demonstrates how the Random Forest model captures these different effects without requiring explicit interaction terms. The effect of age is much more pronounced for non-smokers than for smokers (for whom the baseline probability is already high).

# Final Model Selection and Prediction {#sec-final-model}

```{r final-model-selection, fig.height=4}
# Create a summary table of best models
best_models_df <- data.frame(
  Metric = c("F1 Score", "Accuracy", "AUROC", "AUPRC"),
  Best_Model = c(best_f1_model, best_accuracy_model, best_auroc_model, best_auprc_model),
  Value = c(best_f1_score, best_accuracy_score, best_auroc_score, best_auprc_score)
)

# Format best models table
# kbl(
#   best_models_df,
#   caption = "Best Performing Models by Different Metrics",
#   format = "latex",
#   booktabs = TRUE,
#   align = "c",
#   col.names = c("Evaluation Metric", "Best Model", "Value"),
#   digits = 3
# ) %>%
#   kable_styling(
#     latex_options = c("HOLD_position"),
#     font_size = 10,
#     position = "center"
#   ) %>%
#   row_spec(1, bold = TRUE, background = "#E8F4F8")  # Highlight F1 score row
```

Based on our evaluation, `r best_f1_model` is selected as the optimal model for the following reasons:

1. **Highest F1 Score**: At `r round(best_f1_score, 3)`, this model achieves the best balance between precision and recall, which is critical given our class imbalance.
2. **Strong Overall Performance**: While other models may excel in specific metrics, `r best_f1_model` demonstrates consistently strong performance across multiple evaluation criteria.
3. **Effective Capture of Non-linear Relationships**: As a tree-based ensemble method, `r best_f1_model` effectively captures the complex interactions identified in our EDA.
4. **Consistent Feature Importance**: The model appropriately weighs key predictors (age, smoking status, BMI) in line with our exploratory findings.
5. **Practical Implementation**: This model provides a good balance between predictive power and implementation complexity.

```{r final-prediction, fig.width=9, fig.height=6}
model_map <- list(
  "LASSO Logistic" = "lasso_model",
  "Classification Tree" = "pruned_tree",
  "Random Forest" = "rf_model",
  "XGBoost" = "xgb_model"
)

# Get the appropriate model object based on which model has the best F1 score
selected_model_name <- model_map[[best_f1_model]]
selected_model <- get(selected_model_name)

# Load the provided testing dataset
testing_data <- read.csv("./inst/extdata/A2_testing.csv")

# Ensure proper formatting of predictors with consistent encoding
testing_data <- testing_data %>%
  mutate(
    sex = factor(sex, levels = levels(train_data$sex)),
    smoker = factor(smoker, levels = levels(train_data$smoker)),
    region = factor(region, levels = levels(train_data$region)),
    children = as.integer(children)
  )

# Based on which model is the best, generate predictions accordingly
if(best_f1_model == "Random Forest") {
  # Generate predictions using Random Forest
  final_pred_prob <- predict(rf_model, newdata = testing_data, type = "prob")[, "high"]
  final_predictions <- predict(rf_model, newdata = testing_data)
} else if(best_f1_model == "XGBoost") {
  # Convert testing data to matrix format for XGBoost
  test_matrix <- model.matrix(~ age + sex + bmi + children + smoker + region - 1, data = testing_data)
  # Generate predictions using XGBoost
  final_pred_prob <- predict(xgb_model, test_matrix)
  final_predictions <- ifelse(final_pred_prob > 0.5, "high", "low")
  # Convert to factor with consistent levels
  final_predictions <- factor(final_predictions, levels = c("low", "high"))
} else if(best_f1_model == "Classification Tree") {
  # Generate predictions using Classification Tree
  final_pred_prob <- predict(pruned_tree, newdata = testing_data, type = "prob")[, "high"]
  final_predictions <- predict(pruned_tree, newdata = testing_data)
} else if(best_f1_model == "LASSO Logistic") {
  # Convert testing data to matrix format for LASSO
  test_matrix <- model.matrix(~ age + sex + bmi + children + smoker + region, data = testing_data)[, -1]
  # Generate predictions using LASSO
  final_pred_prob <- predict(lasso_model, newx = test_matrix, type = "response")
  final_predictions <- ifelse(final_pred_prob > 0.5, "high", "low")
  # Convert to factor with consistent levels
  final_predictions <- factor(final_predictions, levels = c("low", "high"))
}

# Create results dataset for visualization
final_results <- testing_data
final_results$predicted_class <- final_predictions
final_results$predicted_prob <- final_pred_prob

# Visualize predictions by smoking status and age
ggplot(final_results, aes(x = age, y = predicted_prob, color = smoker)) +
  geom_point(alpha = 0.7, size = 3) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray50") +
  facet_wrap(~ region, scales = "free_x") +
  scale_color_manual(values = c("no" = "#5ab4ac", "yes" = "#d8b365")) +
  labs(
    title = paste("Predicted Probabilities on External Test Data using", best_f1_model),
    subtitle = paste("Best model by F1 score (", round(best_f1_score, 3), ")", sep=""),
    x = "Age",
    y = "Probability of High Charges",
    color = "Smoker"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    legend.position = "bottom",
    legend.text = element_text(size = 10),
    strip.background = element_rect(fill = "#f0f0f0"),
    strip.text = element_text(face = "bold", size = 10),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )

# Count of predictions by class
prediction_counts <- table(final_predictions)
prediction_percents <- prop.table(prediction_counts) * 100

# Create a clean summary table
prediction_summary <- data.frame(
  Class = names(prediction_counts),
  Percentage = prediction_percents,
  Count = as.integer(prediction_counts)
) 

# Format prediction summary table
# kbl(
#   prediction_summary,
#   caption = paste("Summary of", best_f1_model, "Predictions on External Test Data"),
#   format = "latex",
#   booktabs = TRUE,
#   align = "c",
#   digits = c(0, 1, 0)
# ) %>%
#   kable_styling(
#     latex_options = c("HOLD_position"),
#     font_size = 10,
#     position = "center"
#   )

# Compare external test distribution with training data distribution
distribution_comparison <- data.frame(
  Dataset = c("Training Data", "External Test Data"),
  High_Pct = c(
    round(training_charges_percent["high"], 1),
    round(prediction_percents["high"], 1)
  ),
  Low_Pct = c(
    round(training_charges_percent["low"], 1),
    round(prediction_percents["low"], 1)
  )
)

# Format distribution comparison table
# kbl(
#   distribution_comparison,
#   caption = "Comparison of Class Distribution: Training vs. External Test",
#   format = "latex",
#   booktabs = TRUE,
#   align = "c",
#   col.names = c("Dataset", "High Charges (%)", "Low Charges (%)"),
#   digits = 1
# ) %>%
#   kable_styling(
#     latex_options = c("HOLD_position"),
#     font_size = 10,
#     position = "center"
#   )

```

```{r save-predictions}
# Write predictions to CSV file
write.table(final_predictions, file = "TBSCES001.csv", 
           row.names = FALSE, col.names = FALSE, quote = FALSE)
```

The `r best_f1_model` model was applied to the external test dataset (`r nrow(testing_data)` observations) with these patterns emerging:

1. **Dominant Smoking Effect**: Smokers consistently have higher predicted probabilities across all regions and age groups
2. **Age Gradient**: Probability generally increases with age, more pronounced for non-smokers
3. **Regional Variations**: Some modest regional differences are visible, consistent with our EDA findings
4. **Classification Distribution**: Approximately `r round(prediction_percents["high"], 1)`% of test cases were classified as "high" charges, compared to `r round(training_charges_percent["high"], 1)`% in the training data

The similarity in class distribution between the training and external test datasets (`r round(abs(prediction_percents["high"] - training_charges_percent["high"]), 1)`% difference in "high" charges) suggests the model is generalizing well rather than over or underpredicting high-cost cases.

# Conclusion {#sec-conclusion}

This study developed and evaluated four classification models for predicting insurance charge categories (high/low). Key findings include:

1. **Model Performance**: `r best_f1_model` achieved the highest F1 score (`r round(best_f1_score, 3)`), though other models were only marginally poorer. The strong performance of tree-based models confirms the presence of complex, non-linear relationships in insurance cost factors.

2. **Key Determinants**: Different models rank predictors differently based on their underlying algorithms:
   - **Linear models** (LASSO): Smoking status emerges as the overwhelmingly dominant predictor
   - **Tree-based models** (Random Forest, XGBoost): Age is ranked as the most important variable, while still recognizing smoking as crucial

3. **Non-linear Relationships**: Important non-linear effects were identified, particularly for age, where risk increases dramatically around age 43-46, and for BMI above the clinical obesity threshold (BMI ≥ 30), where risk accelerates rather than increasing linearly.

4. **Interaction Effects**: Significant interactions between smoking status and age were discovered, as visualized in our partial dependence analysis. The effect of age on probability of high charges is much stronger for non-smokers than for smokers, for whom the baseline probability is already high.
